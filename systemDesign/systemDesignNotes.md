# System design

# Database and storage

- Query optimization / Denormalization / Partition/ Sharding
- N+1 Query
- Primary index vs secondary index vs global index

- How to prevent race condition
- How to prevent transaction overwrite (locking / lock free approaches)
- ACID / row lock / distributed lock

- MySQL vs NoSQL vs TimeSeries DB

- Consistent hashing

Storage?
- OLAP vs OLTP
- ElasticSearch
- ELK

# Backend and efficiency

- Speed up on read path
- speed up on write path
- caching strategies
- cache warm up
- sharing: push vs pull

- fanout vs hotspot (celebrity effects)

High availablity:
- Usage too high issues (connection count, CPU, memory, disk, storage, cache)
- CP vs AP (CAP theory)

Pagination (cursor vs offset)

# API design
- RESTful API vs GraphQL API vs gRPC
- API gateway

- talk to 3rd party API (backoff, retry, timeout, circuit breaker, random select, local fallback)

# Messaging system
- Kafka vs AWS SQS vs Redis + celery vs RabbitMQ

- stream processing (Flink)
- Batch processing (spark)

- Event loop (python and nodejs's event loop?)


## Question to answer:

1. How does it make sure messages are not lost?
At least once or at most once?

2. How does it distribute messages?
Multiple worker consume same queue to distribute the load?
Fan out messages: each consumer receive replica of messages?

## Kafka
Reference: https://kafka.apache.org/documentation/

Kafka does not have retry logic

Questions:

1. How Kafka handle offset?
Manage message position consumed in a partition

2. Kafka is disk based? How does it make it fast?

3. Zookeeper used in Kafka?

4. Pull or push model?

5. Livesite issues encountered with Kafka
Hard to manage SSL certificates in master and worker nodes



## RabbitMQ

## Redis Stream
- Reference: https://redis.io/docs/latest/develop/data-types/streams/

- Each entry (or message) is identified by an ID that is generated by Redis by default. The [entry id](https://redis.io/docs/latest/develop/data-types/streams/#entry-ids) format is <millisecondsTime>-<sequenceNumber>

Adding an entry to a stream is O(1). Accessing any single entry is O(n), where n is the length of the ID. Since stream IDs are typically short and of a fixed length, this effectively reduces to a constant time lookup. For details on why, note that streams are implemented as radix trees.


### Redis stream support 3 types of [queries](https://redis.io/docs/latest/develop/data-types/streams/#getting-data-from-streams): querying by range, listening for new items, consumer groups
- Query by range:
XRANGE complexity is O(log(N)) to seek, and then O(M) to return M elements

- Listen for new item: XREAD, it replicates messages to all consumers (fan-out)
A stream can have multiple clients (consumers) waiting for data. Every new item, by default, will be delivered to every consumer that is waiting for data in a given stream. This behavior is different than blocking lists, where each consumer will get a different element. However, the ability to fan out to multiple consumers is similar to Pub/Sub.

- Consumer group: it is completely different from the concept of Kafka consumer group.

    In a single Redis stream, multi coonsumers in a consumer group can receive message out of order. To achieve Kafka partitions, you have to use multiple keys and some sharding system such as Redis Cluster or some other application-specific sharding system.

    Basically Kafka partitions are more similar to using N different Redis keys, while Redis consumer groups are a server-side load balancing system of messages from a given stream to N different consumers.

# Database

## SQL
SQL supports search by any columns without the need of creating indexes but indexes help the performance

## NoSQL
NoSQL doesn't support search by any attributes?

### NoSQL single table design
- Amazon Dynamo DB single table design:
Reference:
    - https://aws.amazon.com/blogs/compute/creating-a-single-table-design-with-amazon-dynamodb/
    - https://dynobase.dev/dynamodb-joins/


- Mongo DB
    - [Data duplication](https://medium.com/mongodb/data-duplication-in-mongodb-and-modern-applications-5a5c1836d114)
    - [The Single Collection Pattern](https://www.mongodb.com/developer/products/mongodb/single-collection-pattern/)

### Amazon Dynamo DB
- Core components: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html
- Issue with Dynamo DB: keys and indexes have to be designed based on query pattern, like how you are going to query the data

# File upload and distribution
- Client upload vs Server upload
- pre-signed URL / Multipart upload on client

# Communication protocols:
- long term connection: websocket vs long polling vs server-sent events
- cookie vs local storage

# Python concurrency
- Process vs thread vs coroutine
- python when to use multiprocessing / threading / asyncio

- ASGI vs WSGI

# Serialization
- Serialization and deserialization
- Bloom Filter